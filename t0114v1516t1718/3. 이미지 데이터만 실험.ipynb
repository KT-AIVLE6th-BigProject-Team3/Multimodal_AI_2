{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227784d7-6e2f-4431-bd05-d16b03103ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\train, 데이터 개수: 35375\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\val, 데이터 개수: 5052\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\test, 데이터 개수: 5052\n",
      "Using device: cuda\n",
      "Epoch 1/30, Train Loss: 0.6063, Val Loss: 2.0877, Val Accuracy: 54.55%\n",
      "Epoch 2/30, Train Loss: 0.5265, Val Loss: 1.8640, Val Accuracy: 59.01%\n",
      "Epoch 3/30, Train Loss: 0.5422, Val Loss: 1.5216, Val Accuracy: 65.91%\n",
      "Epoch 4/30, Train Loss: 0.4856, Val Loss: 1.3996, Val Accuracy: 69.14%\n",
      "Epoch 5/30, Train Loss: 0.4728, Val Loss: 1.7008, Val Accuracy: 66.65%\n",
      "Epoch 6/30, Train Loss: 0.4334, Val Loss: 1.7246, Val Accuracy: 67.18%\n",
      "Epoch 7/30, Train Loss: 0.4237, Val Loss: 2.0633, Val Accuracy: 64.94%\n",
      "Epoch 8/30, Train Loss: 0.4300, Val Loss: 1.9222, Val Accuracy: 67.04%\n",
      "Epoch 9/30, Train Loss: 0.4295, Val Loss: 2.0269, Val Accuracy: 66.15%\n",
      "Epoch 10/30, Train Loss: 0.4249, Val Loss: 1.9699, Val Accuracy: 66.63%\n",
      "Epoch 11/30, Train Loss: 0.4100, Val Loss: 1.8663, Val Accuracy: 68.09%\n",
      "Epoch 12/30, Train Loss: 0.4075, Val Loss: 2.1114, Val Accuracy: 66.47%\n",
      "Epoch 13/30, Train Loss: 0.4056, Val Loss: 2.0955, Val Accuracy: 66.51%\n",
      "Epoch 14/30, Train Loss: 0.4051, Val Loss: 2.2327, Val Accuracy: 65.62%\n",
      "Epoch 15/30, Train Loss: 0.4032, Val Loss: 2.1429, Val Accuracy: 67.22%\n",
      "Epoch 16/30, Train Loss: 0.3998, Val Loss: 2.2502, Val Accuracy: 65.70%\n",
      "Epoch 17/30, Train Loss: 0.3977, Val Loss: 2.2309, Val Accuracy: 65.76%\n",
      "Epoch 18/30, Train Loss: 0.3975, Val Loss: 2.3022, Val Accuracy: 65.52%\n",
      "Epoch 19/30, Train Loss: 0.3984, Val Loss: 2.3193, Val Accuracy: 65.54%\n",
      "Epoch 20/30, Train Loss: 0.3971, Val Loss: 2.3253, Val Accuracy: 65.52%\n",
      "Epoch 21/30, Train Loss: 0.3954, Val Loss: 2.3397, Val Accuracy: 65.40%\n",
      "Epoch 22/30, Train Loss: 0.3947, Val Loss: 2.3381, Val Accuracy: 65.48%\n",
      "Epoch 23/30, Train Loss: 0.3950, Val Loss: 2.3697, Val Accuracy: 65.22%\n",
      "Epoch 24/30, Train Loss: 0.3940, Val Loss: 2.3766, Val Accuracy: 65.22%\n",
      "Epoch 25/30, Train Loss: 0.3928, Val Loss: 2.3968, Val Accuracy: 65.22%\n",
      "Epoch 26/30, Train Loss: 0.3935, Val Loss: 2.3887, Val Accuracy: 65.26%\n",
      "Epoch 27/30, Train Loss: 0.3923, Val Loss: 2.3792, Val Accuracy: 65.32%\n",
      "Epoch 28/30, Train Loss: 0.3938, Val Loss: 2.3850, Val Accuracy: 65.26%\n",
      "Epoch 29/30, Train Loss: 0.3929, Val Loss: 2.3963, Val Accuracy: 65.16%\n",
      "Epoch 30/30, Train Loss: 0.3933, Val Loss: 2.3925, Val Accuracy: 65.22%\n",
      "Test Loss: 2.6985, Test Accuracy: 56.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. 데이터 전처리 및 로더 생성\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_folder, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for root, _, files in os.walk(root_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".bin\"):\n",
    "                    bin_path = os.path.join(root, file)\n",
    "                    json_path = bin_path.replace(\".bin\", \".json\")\n",
    "\n",
    "                    try:\n",
    "                        # BIN 파일 읽기 (120x160 이미지)\n",
    "                        bin_data = np.load(bin_path, allow_pickle=True).astype(np.float32)\n",
    "                        bin_data = bin_data.reshape((120, 160))\n",
    "                    except (UnicodeDecodeError, ValueError):\n",
    "                        print(f\"[Error] BIN 파일 읽기 실패: {bin_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # JSON 파일에서 state 값 읽기\n",
    "                    if os.path.exists(json_path):\n",
    "                        with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                            json_data = json.load(json_file)\n",
    "                            state = json_data.get(\"annotations\", [{}])[0].get(\"tagging\", [{}])[0].get(\"state\", \"N/A\")\n",
    "                    else:\n",
    "                        print(f\"[Warning] JSON 파일을 찾을 수 없습니다: {json_path}\")\n",
    "                        continue\n",
    "\n",
    "                    self.data.append(bin_data)\n",
    "                    self.labels.append(state)\n",
    "\n",
    "        # Label Encoding\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "        print(f\"폴더: {root_folder}, 데이터 개수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # PyTorch 텐서 변환 및 채널 추가\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # (1, 120, 160)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def load_data(data_folder, batch_size=32):\n",
    "    train_dataset = CustomDataset(os.path.join(data_folder, \"train\"))\n",
    "    val_dataset = CustomDataset(os.path.join(data_folder, \"val\"))\n",
    "    test_dataset = CustomDataset(os.path.join(data_folder, \"test\"))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# 2. 모델 정의\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.vit = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=depth,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate  # Transformer 내부 드롭아웃 적용\n",
    "        )\n",
    "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        num_patches = (img_dim_h // patch_size) * (img_dim_w // patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # 추가 드롭아웃 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embed(x).flatten(2).transpose(1, 2)  # [batch_size, num_patches, embed_dim]\n",
    "        x = patches + self.pos_embedding\n",
    "        x = self.vit(x, x)  # [batch_size, num_patches, embed_dim]\n",
    "        x = self.dropout(x.mean(dim=1))  # 드롭아웃 적용\n",
    "        return x  # [batch_size, embed_dim]\n",
    "\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, num_classes, dropout_rate=0.5):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(\n",
    "            img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate  # 전달\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # Classifier 레이어에도 드롭아웃 추가\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3. 학습 함수\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10):\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(100 * correct / total)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f}, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "\n",
    "# 4. 테스트 함수\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_loss / len(test_loader), test_accuracy\n",
    "\n",
    "\n",
    "# 5. 실행\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\"\n",
    "    batch_size = 16\n",
    "    train_loader, val_loader, test_loader = load_data(data_folder, batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = ViTClassifier(\n",
    "        img_dim_h=120, img_dim_w=160, patch_size=16, embed_dim=128,\n",
    "        num_heads=4, depth=8, num_classes=4, dropout_rate=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    num_epochs = 30\n",
    "    train_losses, val_losses, val_accuracies = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs\n",
    "    )\n",
    "\n",
    "    test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f85084-204d-4b20-aa53-ca8bd581867b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba42717-3715-41e3-99c6-a6569e6515e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62a184c-d6a2-451e-b229-d217bcdebe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\train, 데이터 개수: 35375\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\val, 데이터 개수: 5052\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\test, 데이터 개수: 5052\n",
      "Using device: cuda\n",
      "Epoch 1/30, Train Loss: 1.1805, Val Loss: 1.1581, Val Accuracy: 53.92%\n",
      "Epoch 2/30, Train Loss: 1.1730, Val Loss: 1.1584, Val Accuracy: 53.92%\n",
      "Epoch 3/30, Train Loss: 1.1703, Val Loss: 1.1561, Val Accuracy: 53.92%\n",
      "Epoch 4/30, Train Loss: 1.1704, Val Loss: 1.1589, Val Accuracy: 53.92%\n",
      "Epoch 5/30, Train Loss: 1.1706, Val Loss: 1.1556, Val Accuracy: 53.92%\n",
      "Epoch 6/30, Train Loss: 1.1688, Val Loss: 1.1540, Val Accuracy: 53.92%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16308\\1593150746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     train_losses, val_losses, val_accuracies = train_model(\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16308\\1593150746.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16308\\1593150746.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16308\\1593150746.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mpatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [batch_size, num_patches, embed_dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatches\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [batch_size, num_patches, embed_dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 440\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. 데이터 전처리 및 로더 생성\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_folder, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for root, _, files in os.walk(root_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".bin\"):\n",
    "                    bin_path = os.path.join(root, file)\n",
    "                    json_path = bin_path.replace(\".bin\", \".json\")\n",
    "\n",
    "                    try:\n",
    "                        # BIN 파일 읽기 (120x160 이미지)\n",
    "                        bin_data = np.load(bin_path, allow_pickle=True).astype(np.float32)\n",
    "                        bin_data = bin_data.reshape((120, 160))\n",
    "                    except (UnicodeDecodeError, ValueError):\n",
    "                        print(f\"[Error] BIN 파일 읽기 실패: {bin_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # JSON 파일에서 state 값 읽기\n",
    "                    if os.path.exists(json_path):\n",
    "                        with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                            json_data = json.load(json_file)\n",
    "                            state = json_data.get(\"annotations\", [{}])[0].get(\"tagging\", [{}])[0].get(\"state\", \"N/A\")\n",
    "                    else:\n",
    "                        print(f\"[Warning] JSON 파일을 찾을 수 없습니다: {json_path}\")\n",
    "                        continue\n",
    "\n",
    "                    self.data.append(bin_data)\n",
    "                    self.labels.append(state)\n",
    "\n",
    "        # Label Encoding\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "        print(f\"폴더: {root_folder}, 데이터 개수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # PyTorch 텐서 변환 및 채널 추가\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # (1, 120, 160)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def load_data(data_folder, batch_size=32):\n",
    "    train_dataset = CustomDataset(os.path.join(data_folder, \"train\"))\n",
    "    val_dataset = CustomDataset(os.path.join(data_folder, \"val\"))\n",
    "    test_dataset = CustomDataset(os.path.join(data_folder, \"test\"))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# 2. 모델 정의\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.vit = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=depth,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate  # Transformer 내부 드롭아웃 적용\n",
    "        )\n",
    "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        num_patches = (img_dim_h // patch_size) * (img_dim_w // patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # 추가 드롭아웃 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embed(x).flatten(2).transpose(1, 2)  # [batch_size, num_patches, embed_dim]\n",
    "        x = patches + self.pos_embedding\n",
    "        x = self.vit(x, x)  # [batch_size, num_patches, embed_dim]\n",
    "        x = self.dropout(x.mean(dim=1))  # 드롭아웃 적용\n",
    "        return x  # [batch_size, embed_dim]\n",
    "\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, num_classes, dropout_rate=0.5):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(\n",
    "            img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate  # 전달\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # Classifier 레이어에도 드롭아웃 추가\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3. 학습 함수\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10):\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(100 * correct / total)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f}, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "\n",
    "# 4. 테스트 함수\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_loss / len(test_loader), test_accuracy\n",
    "\n",
    "\n",
    "# 5. 실행\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\"\n",
    "    batch_size = 16\n",
    "    train_loader, val_loader, test_loader = load_data(data_folder, batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = ViTClassifier(\n",
    "        img_dim_h=120, img_dim_w=160, patch_size=16, embed_dim=128,\n",
    "        num_heads=4, depth=8, num_classes=4, dropout_rate=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    num_epochs = 30\n",
    "    train_losses, val_losses, val_accuracies = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs\n",
    "    )\n",
    "\n",
    "    test_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11.1",
   "language": "python",
   "name": "tf37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
